{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setting Environment Variables for API Key\n"
      ],
      "metadata": {
        "id": "fT4xlnPuCa52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "0ELSlRHTE5Bz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDo-P7F1QjZccwkU3_6wkAhmROXj1AYYbU\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Google Generative AI Client and Embedding Function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SKzfopb7CmZ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "PeGkgREnFbO9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDo-P7F1QjZccwkU3_6wkAhmROXj1AYYbU\"\n",
        "\n",
        "import google.generativeai as palm\n",
        "\n",
        "palm.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "\n",
        "def get_google_embeddings(texts):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        response = palm.generate_embeddings(\n",
        "            model=\"models/textembedding-gecko-001\",\n",
        "            text=text\n",
        "        )\n",
        "        embeddings.append(response['embeddings'])\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "oZhVYk4MGgZP"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"AIzaSyDo-P7F1QjZccwkU3_6wkAhmROXj1AYYbU\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDo-P7F1QjZccwkU3_6wkAhmROXj1AYYbU\"\n",
        "\n",
        "\n",
        "palm.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "b0KESTMGV6Hs"
      },
      "outputs": [],
      "source": [
        "import request\n",
        "\n",
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading PDF from a URL"
      ],
      "metadata": {
        "id": "sz6z6JOjDtiI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuiXs0bRGrg3",
        "outputId": "ffc81508-043d-48b1-f6cc-4ef417920b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: tables-charts-and-graphs-with-examples-from.pdf\n"
          ]
        }
      ],
      "source": [
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "# Example PDF URLs\n",
        "pdf_urls = [\n",
        "    \"https://www.hunter.cuny.edu/dolciani/pdf_files/workshop-materials/mmcpresentations/tables-charts-and-graphs-with-examples-from.pdf\"\n",
        "]\n",
        "\n",
        "for pdf_url in pdf_urls:\n",
        "    pdf_name = pdf_url.split(\"/\")[-1]\n",
        "    pdf_path = f\"/content/{pdf_name}\"\n",
        "    download_pdf(pdf_url, pdf_path)\n",
        "    print(f\"Downloaded: {pdf_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHqgokJoYOAz",
        "outputId": "9e64c0a9-c183-4464-9b0a-f934fb8bc19d"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.12->langchain-community) (2.27.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and Saving Multiple PDFs"
      ],
      "metadata": {
        "id": "fe1AiC9XD24p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqDVij8AHDN0",
        "outputId": "9b824df5-98c7-4dd9-d057-b15fd3203d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from PyPDF2 import errors\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "\n",
        "        pdf_data = BytesIO(pdf_file.read())\n",
        "\n",
        "\n",
        "        try:\n",
        "            pdf_reader = PdfReader(pdf_data)\n",
        "            raw_text = ''\n",
        "            for page in pdf_reader.pages:\n",
        "                content = page.extract_text()\n",
        "                if content:\n",
        "                    raw_text += content\n",
        "            return raw_text\n",
        "        except errors.PdfReadError as e:\n",
        "            # If PdfReadError occurs (e.g., EOF marker not found), print the error and return empty string\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "            return ''\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependencies and Extracting Text from PDFs"
      ],
      "metadata": {
        "id": "YAjXvzlVD6_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "-7frjEXjTdDf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from PyPDF2 import errors\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to download the PDF\n",
        "def download_pdf(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    with open(save_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "# Function to extract text from the PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as pdf_file:\n",
        "            pdf_reader = PdfReader(BytesIO(pdf_file.read()))  #  BytesIO to handle the PDF data\n",
        "            pdf_text = \"\"\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "                if text:  # Ensure there is text extracted\n",
        "                    pdf_text += text\n",
        "            return pdf_text\n",
        "    except errors.PdfReadError as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QAbvuXqYyD3",
        "outputId": "1d64a358-7970-45b3-d39c-43eaec55552b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (11.0.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Mounting Google Drive and Extracting Text from PDF with pdfplumber"
      ],
      "metadata": {
        "id": "Y5algKoFEEye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XulHl6J2Yr-J",
        "outputId": "7dc8c188-150b-458d-ef00-4e60fb6c6d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total chunks created: 8\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pdfplumber\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'  # Replace with your actual path\n",
        "\n",
        "# Function to extract text from the PDF (using pdfplumber)\n",
        "def extract_pdf_text_with_pdfplumber(pdf_path):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text()\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF with pdfplumber: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Extract text from the PDF\n",
        "pdf_text = extract_pdf_text_with_pdfplumber(pdf_path)\n",
        "\n",
        "# Check if text extraction was successful\n",
        "if pdf_text:\n",
        "    # Initialize the text splitter\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    # Split the extracted text into chunks\n",
        "    texts = text_splitter.split_text(pdf_text)\n",
        "\n",
        "    print(f\"Total chunks created: {len(texts)}\")\n",
        "else:\n",
        "    print(\"No text extracted from the PDF. Text splitting skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing Extracted PDF Text in a List"
      ],
      "metadata": {
        "id": "AvMcMjO6EM_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "cY5TuwK-aS_9"
      },
      "outputs": [],
      "source": [
        "pdf_texts = [pdf_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "-3U_Q482aUDB"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Splitting PDF Text into Chunks and Storing in a List"
      ],
      "metadata": {
        "id": "6KHI0qoUEXaW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv96lvmUablb",
        "outputId": "27db3c73-960a-4166-8fd5-8bdfc348fba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 8\n"
          ]
        }
      ],
      "source": [
        "texts = []  #  an empty list to store all the chunks\n",
        "for pdf_text in pdf_texts:\n",
        "    texts.extend(text_splitter.split_text(pdf_text))\n",
        "print(f\"Total chunks created: {len(texts)}\") #  the total number of chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "SSsV_njBoLCJ"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "\n",
        "# Function to extract text from a specific page\n",
        "def extract_text_from_page(pdf_path, page_number):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            page = pdf.pages[page_number - 1]  # Pages are 0-indexed\n",
        "            return page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from page {page_number}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Text from a Specific Page of a PDF"
      ],
      "metadata": {
        "id": "MgMjLjTDEelV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "G_lZmvEkoONU"
      },
      "outputs": [],
      "source": [
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "page_2_text = extract_text_from_page(pdf_path, 2)\n",
        "\n",
        "\n",
        "if page_2_text:\n",
        "    print(page_2_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting and Displaying Text from Page 2 of the PDF"
      ],
      "metadata": {
        "id": "JX9pRXceFjcJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "dNKiyOcyoTJZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "page_2_text = extract_text_from_page(pdf_path, 2)\n",
        "\n",
        "\n",
        "if page_2_text:\n",
        "    print(page_2_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc8LHlBXKWQ5",
        "outputId": "62275ae0-92ed-48b6-841a-63b9d8bc5a77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages in PDF: 19\n",
            "\n",
            "Page 1 Text:\n",
            "\n",
            "Tables, Charts, and\n",
            "Graphs\n",
            "with Examples from History, Economics,\n",
            "Education, Psychology, Urban Affairs and\n",
            "Everyday Life\n",
            "REVISED: MICHAEL LOLKUS 2018\n",
            "\n",
            "Page 2 Text:\n",
            "\n",
            "\n",
            "\n",
            "Page 3 Text:\n",
            "\n",
            "Tables, Charts, and\n",
            "Graphs Basics\n",
            "\n",
            "Page 4 Text:\n",
            "\n",
            " We use charts and graphs to visualize data.\n",
            " This data can either be generated data, data gathered from\n",
            "an experiment, or data collected from some source.\n",
            " A picture tells a thousand words so it is not a surprise that\n",
            "many people use charts and graphs when explaining data.\n",
            "\n",
            "Page 5 Text:\n",
            "\n",
            "Types of Visual\n",
            "Representations of Data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pdfplumber\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "\n",
        "\n",
        "try:\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        print(f\"Total pages in PDF: {len(pdf.pages)}\")\n",
        "\n",
        "        for i in range(min(5, len(pdf.pages))):\n",
        "            print(f\"\\nPage {i + 1} Text:\\n\")\n",
        "            print(pdf.pages[i].extract_text())\n",
        "except Exception as e:\n",
        "    print(f\"Error opening PDF: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7gl578MNikW",
        "outputId": "76953716-de16-477b-c04c-68e3da4d1354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HTTP_PROXY=\n",
            "env: HTTPS_PROXY=\n"
          ]
        }
      ],
      "source": [
        "%env HTTP_PROXY=\n",
        "%env HTTPS_PROXY="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQKjHYeHNOJc",
        "outputId": "f84e880c-b374-4ced-d822-ce9a3161d10c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8J7EIoEQbVc",
        "outputId": "ab9e9c22-9a31-48a6-b918-1b5f5363983e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Text Extracted from Page 2:\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Raw Text Extracted from Page 2:\\n\")\n",
        "print(page_2_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "6Jh0S-ShVg5-"
      },
      "outputs": [],
      "source": [
        "pattern = r'([A-Za-z\\s]+):?\\s*(\\d+(\\.\\d+)?%)'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Extracting Unemployment Rate Data from Text Using Regular Expressions\""
      ],
      "metadata": {
        "id": "vgruXDB4JzdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cPl0nuvoJzQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "PuO1_l8UVlKc"
      },
      "outputs": [],
      "source": [
        "def extract_unemployment_data_from_text(text):\n",
        "    pattern = r'([A-Za-z\\s]+):?\\s*(\\d+(\\.\\d+)?%)'\n",
        "    matches = re.findall(pattern, text)\n",
        "\n",
        "    data = {}\n",
        "    for match in matches:\n",
        "        degree_type = match[0].strip().lower()  # Normalize the degree type\n",
        "        if degree_type:  # Avoid empty keys\n",
        "            data[degree_type] = match[1]  # Unemployment rate\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Text from PDF using OCR with Tesseract and PyMuPDF\""
      ],
      "metadata": {
        "id": "WFZ4VZSMJ6NH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "gcdP46F1VpGf"
      },
      "outputs": [],
      "source": [
        "from pytesseract import pytesseract\n",
        "\n",
        "def extract_text_with_ocr(pdf_path, page_number):\n",
        "    with fitz.open(pdf_path) as pdf:\n",
        "        page = pdf[page_number - 1]\n",
        "        pix = page.get_pixmap()\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "        return pytesseract.image_to_string(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6eIhfrEhJ7S4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "4YdRQX5nV4l7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "text = page_2_text\n",
        "text = text.replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "Fv8DJqQvVucp"
      },
      "outputs": [],
      "source": [
        "text = text.replace('\\n', ' ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Preprocessing Extracted Text: Removing Newlines for Clean Text\""
      ],
      "metadata": {
        "id": "YWSkren6KAyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMkLAiWkh-cC",
        "outputId": "82021aff-efdd-4780-c75c-80ea755bc314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import fitz\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINAL OUTPUT -\" Extracting DETAILS from PDF Based on User Query\"\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DVSJowlnIMYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "import re\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    all_text = []\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    all_text.append(text)  # Append text from each page to the list\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return all_text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(pdf_texts):\n",
        "    # Initialize the text splitter\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "    texts = []\n",
        "    for pdf_text in pdf_texts:\n",
        "        texts.extend(text_splitter.split_text(pdf_text))\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Function to search query in the text chunks\n",
        "def search_query_in_chunks(texts, query):\n",
        "    query_results = []\n",
        "    query_pattern = re.compile(re.escape(query), re.IGNORECASE)\n",
        "\n",
        "    # Search for the query in each chunk\n",
        "    for chunk_index, text in enumerate(texts):\n",
        "        if query_pattern.search(text):\n",
        "            query_results.append((chunk_index, text))\n",
        "\n",
        "    return query_results\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "\n",
        "# Step 1: Extract text from the PDF\n",
        "pdf_texts = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "if pdf_texts:\n",
        "    # Step 2: Split the extracted text into chunks\n",
        "    texts = split_text_into_chunks(pdf_texts)\n",
        "\n",
        "    # Step 3: Input the query from the user\n",
        "    query = input(\"Enter your query (e.g., 'GDP', 'education', 'charts'): \")\n",
        "\n",
        "    # Step 4: Search for the query in the text chunks\n",
        "    search_results = search_query_in_chunks(texts, query)\n",
        "\n",
        "    if search_results:\n",
        "        print(f\"\\nFound {len(search_results)} matches for your query '{query}' in the document:\\n\")\n",
        "        # Print results for each matched chunk\n",
        "        for idx, (chunk_index, text) in enumerate(search_results):\n",
        "            print(f\"\\n--- Match {idx+1} (Chunk {chunk_index+1}) ---\")\n",
        "            print(text[:500])\n",
        "    else:\n",
        "        print(f\"No matches found for the query '{query}' in the document.\")\n",
        "else:\n",
        "    print(\"Failed to extract text from the PDF.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5fVIo6XFOmU",
        "outputId": "3fc374ae-f59f-44f7-ae40-72ab7ffaf8e0"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query (e.g., 'GDP', 'education', 'charts'): GDP\n",
            "\n",
            "Found 5 matches for your query 'GDP' in the document:\n",
            "\n",
            "\n",
            "--- Match 1 (Chunk 5) ---\n",
            "Table of Yearly U.S. GDP by\n",
            "Industry (in millions of dollars)\n",
            "Source: U.S. Bureau of Labor Statistics\n",
            "Year 2010 2011 2012 2013 2014 2015\n",
            "All Industries 26093515 27535971 28663246 29601191 30895407 31397023\n",
            "Manufacturing 4992521 5581942 5841608 5953299 6047477 5829554\n",
            "Finance,\n",
            "Insurance, Real\n",
            "4522451 4618678 4797313 5031881 5339678 5597018\n",
            "Estate, Rental,\n",
            "Leasing\n",
            "Arts,\n",
            "Entertainment,\n",
            "Recreation, 964032 1015238 1076249 1120496 1189646 1283813\n",
            "Accommodation,\n",
            "and Food Service\n",
            "Other 15614511 16320113\n",
            "\n",
            "--- Match 2 (Chunk 6) ---\n",
            "• The chart below is called a pie chart. It shows what\n",
            "percent “of the pie” a particular category occupies\n",
            "out of the whole.\n",
            "• If total GDP in 2015 is the entire pie, then\n",
            "manufacturing makes up 19% of that pie and finance\n",
            "makes up 18%. Notice that visually speaking, since 19%\n",
            "and 18% are so close to each other in value, their\n",
            "respective slices of the pie are similarly sized.\n",
            "2015 U.S. GDP (in millions of dollars)\n",
            "Manufacturing\n",
            "19%\n",
            "Finance, insurance, real\n",
            "estate, rental, and\n",
            "leasing\n",
            "18% Arts, e\n",
            "\n",
            "--- Match 3 (Chunk 10) ---\n",
            "• The graph below is called a bar graph.\n",
            "• It shows each of the variables independent of each other, each\n",
            "with its own bar.\n",
            "• 2015 GDP for all industries was $31.397023; looking at the graph,\n",
            "the bar for all industries is just above $30.\n",
            "• One is still be able compare each variable with the other by\n",
            "comparing bars.\n",
            "2015 GDP (in trillions of dollars)\n",
            "Other\n",
            "Arts, entertainment, recreation, accommodation, and food\n",
            "services\n",
            "Finance, insurance, real estate, rental, and leasing\n",
            "Manufacturing\n",
            "All indus\n",
            "\n",
            "--- Match 4 (Chunk 11) ---\n",
            "• The graph below is called a line graph. It shows how a variable\n",
            "evolves with respect to another variable. In the line graph below, we\n",
            "show how GDP has evolved by year.\n",
            "35\n",
            "30\n",
            "25\n",
            "20\n",
            "15\n",
            "10\n",
            "5\n",
            "0\n",
            "sralloD\n",
            "7491 0591 3591 6591 9591 2691 5691 8691 1791 4791 7791 0891 3891 6891 9891 2991 5991 8991 1002 4002 7002 0102 3102\n",
            "Yearly Total GDP (in trillions of dollars)\n",
            "Yearly Total GDP\n",
            "Year\n",
            "\n",
            "--- Match 5 (Chunk 12) ---\n",
            "When to use a Line Graph, Pie\n",
            "Chart, or Bar Graph?\n",
            "We use the pie chart here to compare parts of a whole.\n",
            "\n",
            "In our example, we compared components of US GDP.\n",
            "The line chart is useful when you want to show how a\n",
            "\n",
            "variable changes over time. For our purposes, we used it\n",
            "show how GDP changed over time.\n",
            "Bar graphs are good for comparing different groups of\n",
            "\n",
            "variables. We used it to compare different components\n",
            "of US GDP. We did the same with the pie chart;\n",
            "depending on your purposes you may choose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KDPYsHwqIBHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE 2 OUTPUT :- EXTRACTING TABLE OF CONTENTS FROM PAGE 6\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "I-P_06cfIZib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# Function to extract text from a specific page of the PDF\n",
        "def extract_text_from_page(pdf_path, page_number):\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            page = pdf.pages[page_number - 1]  # Pages are 0-indexed\n",
        "            return page.extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from page {page_number}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to parse tabular data from the extracted text\n",
        "def extract_table_from_text(text):\n",
        "    rows = text.split('\\n')  # Split text into lines (rows)\n",
        "    data = [row.split() for row in rows if row.strip()]  # Split rows into columns based on whitespace\n",
        "    return pd.DataFrame(data)  # Convert to DataFrame\n",
        "\n",
        "# Function to respond to the user's query\n",
        "def respond_to_query(query, pdf_path, page_number):\n",
        "    if \"table of contents\" in query.lower():\n",
        "        page_text = extract_text_from_page(pdf_path, page_number)\n",
        "        if page_text:\n",
        "            table_data = extract_table_from_text(page_text)\n",
        "            print(\"Table of Contents from Page 6:\")\n",
        "            print(table_data)\n",
        "        else:\n",
        "            print(\"Could not extract text from the PDF. Check the file path or content.\")\n",
        "    else:\n",
        "        print(\"Sorry, I can only process queries related to the 'table of contents' on Page 6.\")\n",
        "\n",
        "# Input query and file path\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "\n",
        "# User query\n",
        "query = input(\"Please enter your query: \")\n",
        "\n",
        "# Process the query\n",
        "respond_to_query(query, pdf_path, 6)"
      ],
      "metadata": {
        "id": "5uOZYaegxE89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2eed96-5ad2-4a46-8789-9f273beb5b81"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your query: table of contents \n",
            "Table of Contents from Page 6:\n",
            "                 0           1         2         3         4           5  \\\n",
            "0            Table          of    Yearly      U.S.       GDP          by   \n",
            "1         Industry         (in  millions        of  dollars)        None   \n",
            "2          Source:        U.S.    Bureau        of     Labor  Statistics   \n",
            "3             Year        2010      2011      2012      2013        2014   \n",
            "4              All  Industries  26093515  27535971  28663246    29601191   \n",
            "5    Manufacturing     4992521   5581942   5841608   5953299     6047477   \n",
            "6         Finance,        None      None      None      None        None   \n",
            "7       Insurance,        Real      None      None      None        None   \n",
            "8          4522451     4618678   4797313   5031881   5339678     5597018   \n",
            "9          Estate,     Rental,      None      None      None        None   \n",
            "10         Leasing        None      None      None      None        None   \n",
            "11           Arts,        None      None      None      None        None   \n",
            "12  Entertainment,        None      None      None      None        None   \n",
            "13     Recreation,      964032   1015238   1076249   1120496     1189646   \n",
            "14  Accommodation,        None      None      None      None        None   \n",
            "15             and        Food   Service      None      None        None   \n",
            "16           Other    15614511  16320113  16948076  17495515    18318606   \n",
            "\n",
            "           6         7  \n",
            "0       None      None  \n",
            "1       None      None  \n",
            "2       None      None  \n",
            "3       2015      None  \n",
            "4   30895407  31397023  \n",
            "5    5829554      None  \n",
            "6       None      None  \n",
            "7       None      None  \n",
            "8       None      None  \n",
            "9       None      None  \n",
            "10      None      None  \n",
            "11      None      None  \n",
            "12      None      None  \n",
            "13   1283813      None  \n",
            "14      None      None  \n",
            "15      None      None  \n",
            "16  18686638      None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXAMPLE OUTPUT :-EXTRACTING UNEMPLOYMENT DETAILS FROM PAGE 2\n"
      ],
      "metadata": {
        "id": "eJ5i7g9sH6iL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import fitz\n",
        "import re\n",
        "\n",
        "# Extracting text from the PDF with a fallback to OCR\n",
        "def extract_text_with_fallback(pdf_path, page_number):\n",
        "    try:\n",
        "\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = pdf.pages[page_number - 1].extract_text()\n",
        "            if text:\n",
        "                return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error using pdfplumber on page {page_number}: {e}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as pdf:\n",
        "            page = pdf[page_number - 1]\n",
        "            pix = page.get_pixmap()\n",
        "            img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "            return pytesseract.image_to_string(img)\n",
        "    except Exception as e:\n",
        "        print(f\"Error performing OCR on page {page_number}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Extract unemployment rate and earnings data\n",
        "def extract_unemployment_and_earnings_data(text):\n",
        "    degree_data = {}\n",
        "\n",
        "    # Define regex patterns to capture degree names, unemployment rates, and earnings\n",
        "    degree_pattern = re.compile(r\"(Doctoral|Professional|Master's|Bachelor's|Associate's|Some college, no degree|High school diploma)[\\s\\S]*?(\\d+\\.\\d+%)\")\n",
        "    earnings_pattern = re.compile(r\"(\\d{3,4})\\s*median weekly earnings in 2013\")\n",
        "\n",
        "    degree_matches = degree_pattern.findall(text)\n",
        "    earnings_matches = earnings_pattern.findall(text)\n",
        "\n",
        "    # Loop through the degree matches to extract the unemployment rate and earnings\n",
        "    for i, degree_match in enumerate(degree_matches):\n",
        "        degree_name = degree_match[0].strip()\n",
        "        unemployment_rate = degree_match[1].strip()\n",
        "\n",
        "        earnings = earnings_matches[i][0] if i < len(earnings_matches) else \"Data not found\"\n",
        "\n",
        "        degree_data[degree_name] = {\"unemployment_rate\": unemployment_rate, \"earnings\": earnings}\n",
        "\n",
        "    return degree_data\n",
        "\n",
        "def get_degree_data(degree_input, data):\n",
        "    degree_input = degree_input.lower().strip()\n",
        "    for degree in data.keys():\n",
        "        if degree_input in degree.lower():  # Allow partial matching\n",
        "            return data[degree]\n",
        "    return \"No data available for this degree type.\"\n",
        "\n",
        "\n",
        "pdf_path = '/content/drive/MyDrive/Sithafal/Tables, Charts, and Graphs with Examples from History, Economics, Education, Psychology, Urban Affairs and Everyday Life - 2017-2018.pdf'\n",
        "\n",
        "\n",
        "page_2_text = extract_text_with_fallback(pdf_path, 2)\n",
        "if page_2_text:\n",
        "    print(\"Raw Text Extracted from Page 2:\\n\")\n",
        "    print(page_2_text)\n",
        "\n",
        "\n",
        "    degree_data = extract_unemployment_and_earnings_data(page_2_text)\n",
        "    print(\"\\nDegree Data (Unemployment Rate and Earnings):\")\n",
        "    print(degree_data)\n",
        "\n",
        "    # Loop to handle user queries until they choose to exit\n",
        "    while True:\n",
        "        degree_input = input(\"\\nEnter the degree type to get the data (or 'exit' to quit): \").strip()\n",
        "        if degree_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Get data for the degree\n",
        "        result = get_degree_data(degree_input, degree_data)\n",
        "        print(f\"Data for {degree_input}: {result}\")\n",
        "else:\n",
        "    print(\"Failed to extract text from page 2.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFIMChRKrSP-",
        "outputId": "10aa898d-6ea7-4162-c275-157c9c51cdf1"
      },
      "execution_count": 130,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Text Extracted from Page 2:\n",
            "\n",
            "Earnings and Unemployment Rates by Educational Attainment\n",
            "\n",
            "Unemployment rate in 2013 (%) Median weekly earnings in 2013 ($)\n",
            "\n",
            "Doctoral degree\n",
            "Professional degree\n",
            "Master's degree\n",
            "Bachelor's degree\n",
            "Associate's degree\n",
            "Some college, no degree\n",
            "High school diploma\n",
            "\n",
            "Alll workers: 6.1% Alll workers: $827\n",
            "Source: Bureau of Labor Statistics, 2014.\n",
            "\n",
            " \n",
            "\f\n",
            "\n",
            "Degree Data (Unemployment Rate and Earnings):\n",
            "{'Doctoral': {'unemployment_rate': '6.1%', 'earnings': 'Data not found'}}\n",
            "\n",
            "Enter the degree type to get the data (or 'exit' to quit): Master's degree\n",
            "Data for Master's degree: No data available for this degree type.\n",
            "\n",
            "Enter the degree type to get the data (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HUYxD7CdrL2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pk-IvXr1sDCt"
      },
      "execution_count": 130,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}